{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36eb5690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-16 18:56:15.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbachelors_thesis.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: C:\\Users\\natem\\bachelors_thesis\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from bachelors_thesis.modeling.siglabv2.siglabv2 import SigLabV2\n",
    "from bachelors_thesis.modeling.datasets.sigloc_dataset import SigLocDataset\n",
    "from bachelors_thesis.run import lead_sets\n",
    "from bachelors_thesis.utils import count_parameters, confusion_matrix, apply_preprocessors, PRECORDIAL_LEAD_NAMES\n",
    "from bachelors_thesis.data.load_ptbdata_new import PRECORDIAL_LEADS, LIMB_LEADS, AUGMENTED_LEADS, ALL_LEADS\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e2b166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from wandb: 63_siglabv2_inception_gru_2_2\n",
      "+----------------------------------------+------------+\n",
      "|                Modules                 | Parameters |\n",
      "+----------------------------------------+------------+\n",
      "|             encoder.alpha              |     1      |\n",
      "| encoder.cnn_encoder.0.branch1.0.weight |     32     |\n",
      "|  encoder.cnn_encoder.0.branch1.0.bias  |     32     |\n",
      "| encoder.cnn_encoder.0.branch1.1.weight |     32     |\n",
      "|  encoder.cnn_encoder.0.branch1.1.bias  |     32     |\n",
      "| encoder.cnn_encoder.0.branch2.0.weight |     1      |\n",
      "|  encoder.cnn_encoder.0.branch2.0.bias  |     1      |\n",
      "| encoder.cnn_encoder.0.branch2.1.weight |     1      |\n",
      "|  encoder.cnn_encoder.0.branch2.1.bias  |     1      |\n",
      "| encoder.cnn_encoder.0.branch2.3.weight |    160     |\n",
      "|  encoder.cnn_encoder.0.branch2.3.bias  |     32     |\n",
      "| encoder.cnn_encoder.0.branch2.4.weight |     32     |\n",
      "|  encoder.cnn_encoder.0.branch2.4.bias  |     32     |\n",
      "| encoder.cnn_encoder.0.branch3.0.weight |     1      |\n",
      "|  encoder.cnn_encoder.0.branch3.0.bias  |     1      |\n",
      "| encoder.cnn_encoder.0.branch3.1.weight |     1      |\n",
      "|  encoder.cnn_encoder.0.branch3.1.bias  |     1      |\n",
      "| encoder.cnn_encoder.0.branch3.3.weight |    352     |\n",
      "|  encoder.cnn_encoder.0.branch3.3.bias  |     32     |\n",
      "| encoder.cnn_encoder.0.branch3.4.weight |     32     |\n",
      "|  encoder.cnn_encoder.0.branch3.4.bias  |     32     |\n",
      "| encoder.cnn_encoder.0.branch4.1.weight |     32     |\n",
      "|  encoder.cnn_encoder.0.branch4.1.bias  |     32     |\n",
      "| encoder.cnn_encoder.0.branch4.2.weight |     32     |\n",
      "|  encoder.cnn_encoder.0.branch4.2.bias  |     32     |\n",
      "| encoder.cnn_encoder.2.branch1.0.weight |    4096    |\n",
      "|  encoder.cnn_encoder.2.branch1.0.bias  |     32     |\n",
      "| encoder.cnn_encoder.2.branch1.1.weight |     32     |\n",
      "|  encoder.cnn_encoder.2.branch1.1.bias  |     32     |\n",
      "| encoder.cnn_encoder.2.branch2.0.weight |    8192    |\n",
      "|  encoder.cnn_encoder.2.branch2.0.bias  |     64     |\n",
      "| encoder.cnn_encoder.2.branch2.1.weight |     64     |\n",
      "|  encoder.cnn_encoder.2.branch2.1.bias  |     64     |\n",
      "| encoder.cnn_encoder.2.branch2.3.weight |   10240    |\n",
      "|  encoder.cnn_encoder.2.branch2.3.bias  |     32     |\n",
      "| encoder.cnn_encoder.2.branch2.4.weight |     32     |\n",
      "|  encoder.cnn_encoder.2.branch2.4.bias  |     32     |\n",
      "| encoder.cnn_encoder.2.branch3.0.weight |    8192    |\n",
      "|  encoder.cnn_encoder.2.branch3.0.bias  |     64     |\n",
      "| encoder.cnn_encoder.2.branch3.1.weight |     64     |\n",
      "|  encoder.cnn_encoder.2.branch3.1.bias  |     64     |\n",
      "| encoder.cnn_encoder.2.branch3.3.weight |   22528    |\n",
      "|  encoder.cnn_encoder.2.branch3.3.bias  |     32     |\n",
      "| encoder.cnn_encoder.2.branch3.4.weight |     32     |\n",
      "|  encoder.cnn_encoder.2.branch3.4.bias  |     32     |\n",
      "| encoder.cnn_encoder.2.branch4.1.weight |    4096    |\n",
      "|  encoder.cnn_encoder.2.branch4.1.bias  |     32     |\n",
      "| encoder.cnn_encoder.2.branch4.2.weight |     32     |\n",
      "|  encoder.cnn_encoder.2.branch4.2.bias  |     32     |\n",
      "|    encoder.cnn_post_layers.2.weight    |   16384    |\n",
      "|     encoder.cnn_post_layers.2.bias     |    128     |\n",
      "|        encoder.gru.weight_ih_l0        |   49152    |\n",
      "|        encoder.gru.weight_hh_l0        |   49152    |\n",
      "|         encoder.gru.bias_ih_l0         |    384     |\n",
      "|         encoder.gru.bias_hh_l0         |    384     |\n",
      "|        encoder.gru.weight_ih_l1        |   49152    |\n",
      "|        encoder.gru.weight_hh_l1        |   49152    |\n",
      "|         encoder.gru.bias_ih_l1         |    384     |\n",
      "|         encoder.gru.bias_hh_l1         |    384     |\n",
      "|        encoder.attn_proj.weight        |   16384    |\n",
      "|        encoder.attn_vec.weight         |    128     |\n",
      "|          encoder.fc.0.weight           |   16384    |\n",
      "|           encoder.fc.0.bias            |    128     |\n",
      "|       encoder.merge_norm.weight        |    128     |\n",
      "|        encoder.merge_norm.bias         |    128     |\n",
      "|            init_head.weight            |    768     |\n",
      "|             init_head.bias             |     6      |\n",
      "| attention_blocks.0.mha.in_proj_weight  |   49152    |\n",
      "|  attention_blocks.0.mha.in_proj_bias   |    384     |\n",
      "| attention_blocks.0.mha.out_proj.weight |   16384    |\n",
      "|  attention_blocks.0.mha.out_proj.bias  |    128     |\n",
      "|    attention_blocks.0.ffn.0.weight     |   65536    |\n",
      "|     attention_blocks.0.ffn.0.bias      |    512     |\n",
      "|    attention_blocks.0.ffn.2.weight     |   65536    |\n",
      "|     attention_blocks.0.ffn.2.bias      |    128     |\n",
      "|  attention_blocks.0.layernorm1.weight  |    128     |\n",
      "|   attention_blocks.0.layernorm1.bias   |    128     |\n",
      "|  attention_blocks.0.layernorm2.weight  |    128     |\n",
      "|   attention_blocks.0.layernorm2.bias   |    128     |\n",
      "| attention_blocks.1.mha.in_proj_weight  |   49152    |\n",
      "|  attention_blocks.1.mha.in_proj_bias   |    384     |\n",
      "| attention_blocks.1.mha.out_proj.weight |   16384    |\n",
      "|  attention_blocks.1.mha.out_proj.bias  |    128     |\n",
      "|    attention_blocks.1.ffn.0.weight     |   65536    |\n",
      "|     attention_blocks.1.ffn.0.bias      |    512     |\n",
      "|    attention_blocks.1.ffn.2.weight     |   65536    |\n",
      "|     attention_blocks.1.ffn.2.bias      |    128     |\n",
      "|  attention_blocks.1.layernorm1.weight  |    128     |\n",
      "|   attention_blocks.1.layernorm1.bias   |    128     |\n",
      "|  attention_blocks.1.layernorm2.weight  |    128     |\n",
      "|   attention_blocks.1.layernorm2.bias   |    128     |\n",
      "| attention_blocks.2.mha.in_proj_weight  |   49152    |\n",
      "|  attention_blocks.2.mha.in_proj_bias   |    384     |\n",
      "| attention_blocks.2.mha.out_proj.weight |   16384    |\n",
      "|  attention_blocks.2.mha.out_proj.bias  |    128     |\n",
      "|    attention_blocks.2.ffn.0.weight     |   65536    |\n",
      "|     attention_blocks.2.ffn.0.bias      |    512     |\n",
      "|    attention_blocks.2.ffn.2.weight     |   65536    |\n",
      "|     attention_blocks.2.ffn.2.bias      |    128     |\n",
      "|  attention_blocks.2.layernorm1.weight  |    128     |\n",
      "|   attention_blocks.2.layernorm1.bias   |    128     |\n",
      "|  attention_blocks.2.layernorm2.weight  |    128     |\n",
      "|   attention_blocks.2.layernorm2.bias   |    128     |\n",
      "| attention_blocks.3.mha.in_proj_weight  |   49152    |\n",
      "|  attention_blocks.3.mha.in_proj_bias   |    384     |\n",
      "| attention_blocks.3.mha.out_proj.weight |   16384    |\n",
      "|  attention_blocks.3.mha.out_proj.bias  |    128     |\n",
      "|    attention_blocks.3.ffn.0.weight     |   65536    |\n",
      "|     attention_blocks.3.ffn.0.bias      |    512     |\n",
      "|    attention_blocks.3.ffn.2.weight     |   65536    |\n",
      "|     attention_blocks.3.ffn.2.bias      |    128     |\n",
      "|  attention_blocks.3.layernorm1.weight  |    128     |\n",
      "|   attention_blocks.3.layernorm1.bias   |    128     |\n",
      "|  attention_blocks.3.layernorm2.weight  |    128     |\n",
      "|   attention_blocks.3.layernorm2.bias   |    128     |\n",
      "| attention_blocks.4.mha.in_proj_weight  |   49152    |\n",
      "|  attention_blocks.4.mha.in_proj_bias   |    384     |\n",
      "| attention_blocks.4.mha.out_proj.weight |   16384    |\n",
      "|  attention_blocks.4.mha.out_proj.bias  |    128     |\n",
      "|    attention_blocks.4.ffn.0.weight     |   65536    |\n",
      "|     attention_blocks.4.ffn.0.bias      |    512     |\n",
      "|    attention_blocks.4.ffn.2.weight     |   65536    |\n",
      "|     attention_blocks.4.ffn.2.bias      |    128     |\n",
      "|  attention_blocks.4.layernorm1.weight  |    128     |\n",
      "|   attention_blocks.4.layernorm1.bias   |    128     |\n",
      "|  attention_blocks.4.layernorm2.weight  |    128     |\n",
      "|   attention_blocks.4.layernorm2.bias   |    128     |\n",
      "|     classifier.classifier.0.weight     |   32768    |\n",
      "|      classifier.classifier.0.bias      |    256     |\n",
      "|     classifier.classifier.2.weight     |   32768    |\n",
      "|      classifier.classifier.2.bias      |    128     |\n",
      "|     classifier.classifier.4.weight     |    768     |\n",
      "|      classifier.classifier.4.bias      |     6      |\n",
      "+----------------------------------------+------------+\n",
      "Total Trainable Params: 1365845\n",
      "Encoder Trainable Params: 307017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1365845, 307017)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path = \"nateml-maastricht-university/bachelors-thesis\"\n",
    "#run_id = \"83mabq4k\"\n",
    "#version = \"v22\"\n",
    "run_id = \"yh0by5uj\"\n",
    "version = \"v30\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the torch model from wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "run = api.run(f\"{project_path}/{run_id}\")\n",
    "config = dict(run.config)\n",
    "\n",
    "# Get the run name\n",
    "run_name = run.name\n",
    "\n",
    "artifact = api.artifact(f\"{project_path}/{run_name}:{version}\")\n",
    "artifact_path = artifact.download()\n",
    "\n",
    "# Convert config to omegaconf\n",
    "cfg = OmegaConf.create(config)\n",
    "\n",
    "# Load the model\n",
    "checkpoint = torch.load(artifact_path + f\"/{run_name}.pth\", map_location=device)\n",
    "\n",
    "# Cast the model to the correct type\n",
    "model = SigLabV2(cfg.model).to(device)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()  # Put into evaluation mode\n",
    "\n",
    "# Count number of parameters\n",
    "print(f\"Loaded model from wandb: {run_name}\")\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a46a482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2183, 6, 1000])\n",
      "(2183, 6)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = Path(\"../../\" + cfg.dataset.path)\n",
    "if OmegaConf.select(cfg, \"dataset.only_precordial\"):\n",
    "    dataset_path = dataset_path / \"precordial\"\n",
    "else:\n",
    "    dataset_path = dataset_path / \"all\"\n",
    "dataset_path = dataset_path.resolve()\n",
    "\n",
    "val_data = np.load(dataset_path / \"val.npy\")\n",
    "test_data = np.load(dataset_path / \"test.npy\")\n",
    "\n",
    "# Apply preprocessors\n",
    "val_data = apply_preprocessors(val_data,\n",
    "                               cfg.dataset.sampling_rate,\n",
    "                               cfg.preprocessor_group.preprocessors)\n",
    " \n",
    "# Convert to torch tensor\n",
    "val_data = torch.from_numpy(val_data).float().to(device)\n",
    "val_data = val_data.permute(0, 2, 1)\n",
    "print(val_data.shape)\n",
    "\n",
    "test_data = torch.from_numpy(test_data).float().to(device)\n",
    "test_data = test_data.permute(0, 2, 1)\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "lead_filter = lead_sets[OmegaConf.select(cfg, \"run.leads\", default=\"precordial\")]\n",
    "dataset = SigLocDataset(val_data, filter_leads=lead_filter)\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False)\n",
    "\n",
    "# I need to reorder val_data to match the order of the labels in the dataset\n",
    "if OmegaConf.select(cfg, \"dataset.only_precordial\") or OmegaConf.select(cfg, \"dataset.only_precordial\") is None:\n",
    "    val_data = val_data[:, [PRECORDIAL_LEADS.index(lead) for lead in lead_filter], :]\n",
    "else:\n",
    "    val_data = val_data[:, [ALL_LEADS.index(lead) for lead in lead_filter], :]\n",
    "\n",
    "# Load metadata\n",
    "meta_val = pd.read_csv(dataset_path / \"meta_val.csv\")\n",
    "meta_test = pd.read_csv(dataset_path / \"meta_test.csv\")\n",
    "meta = meta_val.copy()\n",
    "meta['scp_codes'] = meta['scp_codes'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "def codes_above_threshold(code_dict, thr=THRESHOLD):\n",
    "    return {code for code, prob in code_dict.items() if prob >= thr}\n",
    "\n",
    "meta[\"present_codes\"] = meta[\"scp_codes\"].apply(lambda x: codes_above_threshold(x, THRESHOLD))\n",
    "\n",
    "# Treat diagnostic superclass as lists\n",
    "meta[\"diagnostic_superclass\"] = meta[\"diagnostic_superclass\"].apply(lambda x: ast.literal_eval(x))\n",
    "# Now convert to a set\n",
    "meta[\"diagnostic_superclass\"] = meta[\"diagnostic_superclass\"].apply(lambda x: set(x))\n",
    "\n",
    "c = cfg.model.num_classes\n",
    "logits = np.zeros((len(dataset), c, c))\n",
    "init_logits = np.zeros((len(dataset), c, c))\n",
    "targets = np.zeros((len(dataset), c))\n",
    "\n",
    "for idx, (signals, lead_labels) in enumerate(dataloader):\n",
    "    signals = signals.to(device)\n",
    "    lead_labels = lead_labels.to(device)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        these_logits = model(signals)\n",
    "        logits[(idx * batch_size):(idx * batch_size + batch_size)] = these_logits.cpu().numpy()\n",
    "        targets[(idx * batch_size):(idx * batch_size + batch_size)] = lead_labels.cpu().numpy()\n",
    "\n",
    "predictions = logits.argmax(axis=1)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47bf5963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ee1f231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swap accuracy: 0.9789\n"
     ]
    }
   ],
   "source": [
    "def detect_swap(predictions, lead1, lead2):\n",
    "    \"\"\"\n",
    "    Detects if the model is predicting V1 and V2 in the wrong order.\n",
    "    This is done by checking if the predicted class for V1 is V2 and\n",
    "    the predicted class for V2 is V1.\n",
    "    \"\"\"\n",
    "    i1 = PRECORDIAL_LEADS.index(lead1)\n",
    "    i2 = PRECORDIAL_LEADS.index(lead2)\n",
    "    swap = (predictions[:, i1] == i1) & (predictions[:, i2] == i2)\n",
    "    return swap\n",
    "\n",
    "# Calculate the swap accuracy\n",
    "swaps = detect_swap(predictions, \"V1\", \"V6\")\n",
    "# Calculate the swap accuracy\n",
    "swap_accuracy = np.mean(swaps)\n",
    "print(f\"Swap accuracy: {swap_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "087b612f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  23,   34,   35,   36,   52,   78,   83,   92,  122,  185,  198,\n",
       "         215,  239,  304,  387,  463,  468,  471,  483,  486,  524,  538,\n",
       "         578,  580,  581,  632,  683,  686,  689,  717,  727,  749,  837,\n",
       "         848,  859,  872,  881,  883,  905,  936,  948,  955,  956,  961,\n",
       "        1004, 1056, 1058, 1074, 1077, 1198, 1206, 1215, 1218, 1220, 1240,\n",
       "        1250, 1259, 1290, 1293, 1296, 1320, 1379, 1395, 1407, 1468, 1486,\n",
       "        1510, 1546, 1550, 1595, 1614, 1632, 1647, 1753, 1754, 1789, 1862,\n",
       "        1911, 2004, 2041, 2177]),)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(~swaps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
